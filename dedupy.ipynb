{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a277e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import re\n",
    "import nltk\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "\n",
    "# Ensure NLTK data is present (run once):\n",
    "# python -m nltk.downloader punkt stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def preprocess(text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Normalize and tokenize text:\n",
    "      - Lowercase\n",
    "      - Remove HTML tags\n",
    "      - Remove non-alphanumeric chars\n",
    "      - Tokenize\n",
    "      - Remove stopwords & short tokens\n",
    "    \"\"\"\n",
    "    text = (text or \"\").lower()\n",
    "    text = re.sub(r\"<.*?>\", \" \", text)\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return [t for t in tokens if t not in STOPWORDS and len(t) > 1]\n",
    "\n",
    "\n",
    "def detect_exact_duplicates(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Return a Series mapping each index to the article_id of its first exact duplicate.\n",
    "    \"\"\"\n",
    "    hash_map: dict[str, str] = {}\n",
    "    result = {}\n",
    "    for idx, text in df['normalized'].items():\n",
    "        h = hashlib.sha256(text.encode('utf-8')).hexdigest()\n",
    "        first = hash_map.get(h)\n",
    "        if first is None:\n",
    "            first = df.at[idx, 'article_id']\n",
    "            hash_map[h] = first\n",
    "        result[idx] = first\n",
    "    return pd.Series(result)\n",
    "\n",
    "\n",
    "def detect_near_duplicates(\n",
    "    df: pd.DataFrame,\n",
    "    num_perm: int,\n",
    "    threshold: float,\n",
    "    shingle_size: int\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Return a Series mapping each index to the representative article_id\n",
    "    of its near-duplicate cluster (via MinHash + LSH).\n",
    "    \"\"\"\n",
    "    lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)\n",
    "    minhashes: dict[int, MinHash] = {}\n",
    "\n",
    "    # Build signatures & insert into LSH\n",
    "    for idx, tokens in df['tokens'].items():\n",
    "        shingles = (\n",
    "            [' '.join(tokens[i:i+shingle_size])\n",
    "             for i in range(len(tokens)-shingle_size+1)]\n",
    "            if len(tokens) >= shingle_size else tokens\n",
    "        )\n",
    "        m = MinHash(num_perm=num_perm)\n",
    "        for s in set(shingles):\n",
    "            m.update(s.encode('utf-8'))\n",
    "        minhashes[idx] = m\n",
    "        lsh.insert(idx, m)\n",
    "\n",
    "    # Cluster and pick representative by smallest integer ID\n",
    "    cluster_map: dict[int, str] = {}\n",
    "    visited = set()\n",
    "    for idx in df.index:\n",
    "        if idx in visited:\n",
    "            continue\n",
    "        group = set(lsh.query(minhashes[idx]))\n",
    "        # choose rep = min as integer\n",
    "        rep_id = str(\n",
    "            min(int(df.at[i, 'article_id']) for i in group)\n",
    "        )\n",
    "        for i in group:\n",
    "            cluster_map[i] = rep_id\n",
    "            visited.add(i)\n",
    "    return pd.Series(cluster_map)\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='News Article Deduplication')\n",
    "    parser.add_argument('--input',  required=True,\n",
    "                        help='Path to input CSV')\n",
    "    parser.add_argument('--output', required=True,\n",
    "                        help='Path for output CSV')\n",
    "    parser.add_argument('--threshold', type=float, default=0.8,\n",
    "                        help='Jaccard sim. threshold for near duplicates')\n",
    "    parser.add_argument('--shingle-size', type=int, default=5,\n",
    "                        help='Number of words per shingle (default: 5)')\n",
    "    parser.add_argument('--num-perm', type=int, default=128,\n",
    "                        help='Number of permutations for MinHash (default: 128)')\n",
    "    parser.add_argument('--block-by-date', action='store_true',\n",
    "                        help='Only compare near duplicates within same publication_date')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    df = pd.read_csv(args.input, dtype={'article_id': str})\n",
    "\n",
    "    # Preprocess\n",
    "    df['tokens'] = (\n",
    "        df['title'].fillna('') + ' ' + df['content_snippet'].fillna('')\n",
    "    ).apply(preprocess)\n",
    "    df['normalized'] = df['tokens'].apply(lambda toks: ' '.join(toks))\n",
    "\n",
    "    # Exact duplicates\n",
    "    df['exact_duplicate_of'] = detect_exact_duplicates(df)\n",
    "\n",
    "    # Near duplicates, optionally blocked by date\n",
    "    near = pd.Series(dtype=str)\n",
    "    if args.block_by_date and 'publication_date' in df:\n",
    "        for date, sub in df.groupby('publication_date'):\n",
    "            near_sub = detect_near_duplicates(\n",
    "                sub,\n",
    "                num_perm=args.num_perm,\n",
    "                threshold=args.threshold,\n",
    "                shingle_size=args.shingle_size\n",
    "            )\n",
    "            near = near.append(near_sub)\n",
    "    else:\n",
    "        near = detect_near_duplicates(\n",
    "            df,\n",
    "            num_perm=args.num_perm,\n",
    "            threshold=args.threshold,\n",
    "            shingle_size=args.shingle_size\n",
    "        )\n",
    "    df['near_duplicate_of'] = near.sort_index()\n",
    "\n",
    "    # Select only the required output columns\n",
    "    output_cols = [\n",
    "        'article_id', 'title', 'publication_date',\n",
    "        'source_url', 'content_snippet',\n",
    "        'exact_duplicate_of', 'near_duplicate_of'\n",
    "    ]\n",
    "    df.to_csv(args.output, columns=output_cols, index=False)\n",
    "    print(f'âœ… Deduplication complete. Output: {args.output}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
